{"cells":[{"metadata":{"_uuid":"fdf78cc530ebd297901ba4a3e2aa8326ae1b8f28","_cell_guid":"3b99a71d-bf76-4d37-85ac-fb7225186b99"},"cell_type":"markdown","source":"# Colorization of Black and White Photos Using Neural Networks (Keras Implementation)\n"},{"metadata":{"collapsed":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom IPython.display import display, Image\nfrom matplotlib.pyplot import imshow\nfrom keras.layers import Conv2D, UpSampling2D, InputLayer\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import img_to_array, load_img\nfrom skimage.color import lab2rgb, rgb2lab\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/rural_and_urban_photos/train/urban\"))\n\n\n\n# Any results you write to the current directory are saved as output.\n\nINPUT_IMAGE_SRC = '../input/rural_and_urban_photos/train/urban/urban_11.jpeg'","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"d407fae0ecd1c9903019a6dd0c8fe9a68794172d","_cell_guid":"1f002a38-7a84-4cf7-9d7a-1904fa3af587","trusted":false},"cell_type":"code","source":"display(Image(INPUT_IMAGE_SRC, width=225))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"832ef6ff08cbdbf0fcc5fb4f06503fd4834b214a","_cell_guid":"29519e4f-4864-4270-b137-c247eec86058"},"cell_type":"markdown","source":"> The Lab color space describes mathematically all perceivable colors in the three dimensions **L for lightness** and **a** and **b** for the color components **green–red** and **blue–yellow**.\n\nand the reason why we want to use it instead of RGB\n\n> Unlike the RGB and CMYK color models, Lab color is **designed to approximate human vision**. It aspires to perceptual uniformity, and its L component closely matches human perception of lightness, although it does not take the Helmholtz–Kohlrausch effect into account. Thus, it can be used to make accurate color balance corrections by modifying output curves in the a and b components, or to adjust the lightness contrast using the L component. In RGB or CMYK spaces, which **model the output of physical devices rather than human visual perception**, these transformations can be done only with the help of appropriate blend modes in the editing application.\n\nvia https://en.wikipedia.org/wiki/Lab_color_space"},{"metadata":{"_uuid":"5d7cf8ceb066fc2120a8b6d9128f7f1961b10cda","_cell_guid":"49360ea6-11bb-4296-bcf5-f605fd373949"},"cell_type":"markdown","source":"Let's load the image and turn its **RBG** representation into **Lab**. Before doing that, we should normalize array represenation of the image. RGB values range from (0, 255), but for our purposes, it is useful to constraint them to the (0,1) range. We can do so, by diving the R, G, and B values of each pixes by the maximum (255). Having done that, we can proceed covenrting the RGB image to Lab. The resulting numpy array has a shape of `(width, height, 3)`, the 3 standing for the three different layers we end up with. The first layer is our Lightness component, or put simply the black and white representation we would use as inputs  in our training. The rest of the layers represent the respective color mappings and will be used as our training outputs."},{"metadata":{"collapsed":true,"_uuid":"e21c6d4740d558421e5138ebf809f09b33ff6a8c","_cell_guid":"efbaf7dd-4842-490b-a1f9-1b3f3ef823d3","trusted":false},"cell_type":"code","source":"image = img_to_array(load_img(INPUT_IMAGE_SRC, target_size=(200,200))) / 255\nlab_image = rgb2lab(image)\nlab_image.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2aecdba9a1e6aaaf38860ed06b45d0e92e0e5705","_cell_guid":"1b74a210-2cfd-4b02-9ab3-b80be7bd9f90"},"cell_type":"markdown","source":"One more normalization step is needed, before we can extract the data. The color values  in the different Lab layers are in various ranges, as follows:\n* L: (0, 100)\n* a & b: (-128, 128)\nvia http://www.colourphil.co.uk/lab_lch_colour_space.shtml\n\nFor our purposes, we would want them all to be normalized and in the \n\n"},{"metadata":{"collapsed":true,"_uuid":"ed26c451e0a4632ad38d84975eb3b0f4bd64ad7c","_cell_guid":"9a590c30-23d2-42f4-8c8d-bb54ab3416b9","trusted":false},"cell_type":"code","source":"lab_image_norm = (lab_image + [0, 128, 128]) / [100, 255, 255]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b5a9b7191c8761db110dfaa9a5844fe34c4961c","_cell_guid":"4b490660-c770-42d5-b44f-935891869f56"},"cell_type":"markdown","source":"Having turned the image into an Lab represantion, let's define the inputs and outputs. The input in our case will be the black and white layer (`lab_image[:,:,0]`)"},{"metadata":{"collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"# The input will be the black and white layer\nX = lab_image_norm[:,:,0]\n# The outpts will be the ab channels\nY = lab_image_norm[:,:,1:]\n\n# The Conv2D layer we will use later expects the inputs and training outputs to be of the following format:\n# (samples, rows, cols, channels), so we need to do some reshaping\n# https://keras.io/layers/convolutional/\nX = X.reshape(1, X.shape[0], X.shape[1], 1)\nY = Y.reshape(1, Y.shape[0], Y.shape[1], 2)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"bc1735c3f7d04dae89eda105c03bc9842d648682","_cell_guid":"c78d8788-afe1-49ee-8670-fa2c852b5304","trusted":false},"cell_type":"code","source":"model = Sequential()\nmodel.add(InputLayer(input_shape=(None, None, 1)))\nmodel.add(Conv2D(8, (3, 3), activation='relu', padding='same', strides=2))\nmodel.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\nmodel.add(Conv2D(16, (3, 3), activation='relu', padding='same', strides=2))\nmodel.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\nmodel.add(Conv2D(32, (3, 3), activation='relu', padding='same', strides=2))\nmodel.add(UpSampling2D((2, 2)))\nmodel.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\nmodel.add(UpSampling2D((2, 2)))\nmodel.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\nmodel.add(UpSampling2D((2, 2)))\nmodel.add(Conv2D(2, (3,3), activation='tanh', padding='same'))\n\n# Finish model\nmodel.compile(optimizer='rmsprop', loss='mse')\nmodel.fit(x=X, y=Y, batch_size=1, epochs=1000, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"27f32148f9879e96af9aa962e2ae56a1ae20d72b","_cell_guid":"7a6c6bf2-0994-46e8-bf67-821cdc8dde01","trusted":false},"cell_type":"code","source":"model.evaluate(X, Y, batch_size=1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"1d32463613adde48fcd59132b0541ad4ad86a51c","_cell_guid":"9243658d-56c9-43ae-9e35-bedde830d3d7","trusted":false},"cell_type":"code","source":"output = model.predict(X)\ncur = np.zeros((200, 200, 3))\ncur[:,:,0] = X[0][:,:,0]\ncur[:,:,1:] = output[0]\n\n\n#imshow(rgb_image.astype('float32'))\n\n#cur = (cur * [100, 255, 255]) - [0 ,128, 128]\n#output\n\n#rgb_image *= [100, 255, 255]\ncur = (cur * [100, 255, 255]) - [0, 128, 128]\nrgb_image = lab2rgb(cur)\nimshow(rgb_image)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"88546056e1d54bd2db3f13680fed683e06921eb9","_cell_guid":"a81912df-e039-421c-8c9b-4c8c721a4e74","trusted":false},"cell_type":"markdown","source":"# References\nhttps://github.com/microic/niy/tree/master/examples/colorizing_photos"}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}